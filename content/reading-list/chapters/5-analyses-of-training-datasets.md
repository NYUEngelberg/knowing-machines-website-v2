This section highlights works that analyze training datasets from a variety of methodological and theoretical perspectives. While we understand that many of the titles that span across the major headings in this reading list involve some form of ‚Äúdataset analysis,‚Äù we highlight in this particular section studies in which the analysis itself comprises the thrust of the article/chapter/work. The works in this section focus primarily on the _details of the analysis_ as opposed to conducting an analysis as a preliminary step to introduce a more central argument or intervention.¬†

### **a. Sociotechnical & Critical Studies**

This subsection focuses on articles and chapters that approach their analyses of training datasets grounded in frameworks primarily taken from critical studies or science and technology studies.¬†

*   Bao, M., Zhou, A., Zottola, S. A., Brubach, B., Desmarais, S., Horowitz, A., Lum, K., & Venkatasubramanian, S. (2021). It‚Äôs COMPASlicated: The Messy Relationship between RAI Datasets and Algorithmic Fairness Benchmarks. _ArXiv_. [https://arxiv.org/abs/2106.05498](https://arxiv.org/abs/2106.05498)
*   Busch, L. (2014). A Dozen Ways to Get Lost in Translation: Inherent Challenges in Large Scale Data Sets. _International Journal of Communication_, _8_, 1727-1744. [https://ijoc.org/index.php/ijoc/article/view/2160](https://ijoc.org/index.php/ijoc/article/view/2160)
*   Coleman, C. N. (2020). Managing Bias When Library Collections Become Data. _International Journal of Librarianship, 5_(1), 8‚Äì19. [https://doi.org/10.23974/ijol.2020.vol5.1.162](https://doi.org/10.23974/ijol.2020.vol5.1.162)
*   Coveney, P. V., Dougherty, E. R., & Highfield, R. R. (2016). Big Data Need Big Theory Too. _Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374_, 1-11. [https://doi.org/10.1098/rsta.2016.0153](https://doi.org/10.1098/rsta.2016.0153)
*   Feinberg, M. (2017). A Design Perspective on Data. _Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems_, 2952‚Äì2963. [https://doi.org/10.1145/3025453.3025837](https://doi.org/10.1145/3025453.3025837)
*   Jo, E. S., & Gebru, T. (2020). Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning. _Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency_, 306‚Äì316. [https://doi.org/10.1145/3351095.3372829](https://doi.org/10.1145/3351095.3372829)
*   Prabhu, V. U., & Birhane, A. (2020). Large Image Datasets: A Pyrrhic Win for Computer Vision? _ArXiv_. [http://arxiv.org/abs/2006.16923](http://arxiv.org/abs/2006.16923)
*   Richardson, R., Schultz, J. M., & Crawford, K. (2019). Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice. _NYU Law Review_, _94_(15), 15‚Äì55. [https://www.nyulawreview.org/online-features/dirty-data-bad-predictions-how-civil-rights-violations-impact-police-data-predictive-policing-systems-and-justice/](https://www.nyulawreview.org/online-features/dirty-data-bad-predictions-how-civil-rights-violations-impact-police-data-predictive-policing-systems-and-justice/)
*   Sambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P. K., & Aroyo, L. (2021). ‚ÄúEveryone Wants to Do the Model Work, Not the Data Work‚Äù: Data Cascades in High-Stakes AI. _Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems_, 1-15. [https://doi.org/10.1145/3411764.3445518](https://doi.org/10.1145/3411764.3445518)
*   Scheuerman, M. K., Denton, E., & Hanna, A. (2021). Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. _ArXiv_. [https://doi.org/10.1145/3476058](https://doi.org/10.1145/3476058)
*   Scheuerman, M. K., Paul, J. M., & Brubaker, J. R. (2019). How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial Analysis Services. _Proceedings of the ACM on Human-Computer Interaction, 3(CSCW)_, 1-33. [https://doi.org/10.1145/3359246](https://doi.org/10.1145/3359246)
*   Scheuerman, M. K., Wade, K., Lustig, C., & Brubaker, J. R. (2020). How We‚Äôve Taught Algorithms to See Identity: Constructing Race and Gender in Image Databases for Facial Analysis. _Proceedings of the ACM on Human-Computer Interaction, 4(CSCW1)_, 1-35. [https://doi.org/10.1145/3392866](https://doi.org/10.1145/3392866)
*   Smits, T., & Wevers, M. (2021). The Agency of Computer Vision Models as Optical Instruments. _Visual Communication_, 1-21. [https://doi.org/10.1177/1470357221992097](https://doi.org/10.1177/1470357221992097)
*   Stevens, N., & Keyes, O. (2021). Seeing infrastructure: Race, Facial Recognition and the Politics of Data. _Cultural Studies, 35_(4-5), 833-853. [https://doi.org/10.1080/09502386.2021.1895252](https://doi.org/10.1080/09502386.2021.1895252)
*   Trewin, S. (2018). AI Fairness for People with Disabilities: Point of View. _ArXiv_. [http://arxiv.org/abs/1811.10670](http://arxiv.org/abs/1811.10670)

### **b. Technical Approaches to Studying Datasets**

Here, we introduce works that detail ‚Äútechnical‚Äù methods for the study of datasets. While the titles housed under the following subsection 5c, ‚ÄúTechnical Audits,‚Äù deal with the _investigative_ technical analysis of _particular_ datasets, the works in this subsection are more concerned with introducing technical methods to approach the study of datasets and their particular components. Many of these studies do contain audit-style analyses, but we differentiate them from subsection 5c because their focus is on introducing or using technical methods for dataset analysis in general, as opposed to dissecting various components of particular datasets.¬†

*   Balayn, A., Kulynych, B., & Guerses, S. (2021). Exploring Data Pipelines through the Process Lens: A Reference Model for Computer Vision. _ArXiv_. [https://arxiv.org/abs/2107.01824](https://arxiv.org/abs/2107.01824)
*   Bender, E. M., Gebru, T., McMillan-Major, A., & Mitchell, M. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú. _FAccT_. [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922)
*   Blodgett, S. L., Lopez, G., Olteanu, A., Sim, R., & Wallach, H. (202). Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets. _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics_, _1_, 1004-1015.¬†
*   Cheng, V., Suriyakumar, V., Dullerud, N., Joshi, S., & Ghassemi, M. (2021). Can You Fake It Until You Make It?: Impacts of Differentially Private Synthetic Data on Downstream Classification Fairness. _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, 149-160. [https://doi.org/10.1145/3442188.3445879](https://doi.org/10.1145/3442188.3445879)
*   Gardner, M., Merrill, W., Dodge, J., Peters, M. E., Ross, A., Singh, S., & Smith, N. A. (2021). Competency Problems: On Finding and Removing Artifacts in Language Data. _ArXiv_. [https://arxiv.org/abs/2104.08646](https://arxiv.org/abs/2104.08646)
*   Fabbrizzi, S., Papadopoulos, S., Ntoutsi, E., & Kompatsiaris, Y. (2021). A Survey on Bias in Visual Datasets. _ArXiv_. [https://arxiv.org/abs/2107.07919](https://arxiv.org/abs/2107.07919)
*   Hirota, Y., Nakashima, Y., & Garcia, N. (2022). Gender and Racial Bias in Visual Question Answering Datasets. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 1280‚Äì1292. [https://doi.org/10.1145/3531146.3533184](https://doi.org/10.1145/3531146.3533184)
*   Hutchinson, B., Rostamzadeh, N., Greer, C., Heller, K., & Prabhakaran, V. (2022). Evaluation Gaps in Machine Learning Practice. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 1859‚Äì1876. [https://doi.org/10.1145/3531146.3533233](https://doi.org/10.1145/3531146.3533233)
*   Jung, T., Kang, D., Mentch, L., & Hovy, E. (2019). Earlier Isn‚Äôt Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization. _ArXiv_. [http://arxiv.org/abs/1908.11723](http://arxiv.org/abs/1908.11723)
*   Kilgarriff, A., & Grefenstette, G. (2003). Introduction to the Special Issue on the Web as Corpus. _Computational Linguistics_, _29_(3), 333‚Äì348. [https://doi.org/10.1162/089120103322711569](https://doi.org/10.1162/089120103322711569)
*   Koesten, L., Vougiouklis, P., Simperl, E., & Groth, P. (2020). Dataset Reuse: Toward Translating Principles to Practice. _Patterns, 1_(8), 100136. [https://doi.org/10.1016/j.patter.2020.100136](https://doi.org/10.1016/j.patter.2020.100136)
*   Laranjeira da Silva, C., Macedo, J., Avila, S., & dos Santos, J. (2022). Seeing without Looking: Analysis Pipeline for Child Sexual Abuse Datasets. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 2189‚Äì2205. [https://doi.org/10.1145/3531146.3534636](https://doi.org/10.1145/3531146.3534636)
*   Madras, D., Creager, E., Pitassi, T., & Zemel, R. (2019). Fairness through Causal Awareness: Learning Causal Latent-Variable Models for Biased Data. _Proceedings of the Conference on Fairness, Accountability, and Transparency_, 349‚Äì358. [https://doi.org/10.1145/3287560.3287564](https://doi.org/10.1145/3287560.3287564)
*   Moreno-Torres, J. G., Raeder, T., Alaiz-Rodr√≠guez, R., Chawla, N. V., & Herrera, F. (2012). A Unifying View on Dataset Shift in Classification. _Pattern Recognition_, _45_(1), 521‚Äì530. [https://doi.org/10.1016/j.patcog.2011.06.019](https://doi.org/10.1016/j.patcog.2011.06.019)
*   Olson, R. S., La Cava, W., Orzechowski, P., Urbanowicz, R. J., & Moore, J. H. (2017). PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison. _BioData Mining_, _10_(36). [https://doi.org/10.1186/s13040-017-0154-4](https://urldefense.com/v3/__https://doi.org/10.1186/s13040-017-0154-4__;!!LIr3w8kk_Xxm!8b-dYz-Rsa7iMiBfoUZfFcOUl8g_my0VNHnOmE-6dmTJHkV49S2av1if6lTq4Vgs$)
*   Rabanser, S., G√ºnnemann, S., & Lipton, Z. C. (2019). Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift. _ArXiv_. [http://arxiv.org/abs/1810.11953](http://arxiv.org/abs/1810.11953)
*   Rieke, A., Sutherland, V., Svirsky, D., & Hsu, M. (2022). Imperfect Inferences: A Practical Assessment. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 767-777. [https://doi.org/10.1145/3531146.3533140](https://doi.org/10.1145/3531146.3533140)
*   Straw, I., & Callison-Burch, C. (2020). Artificial Intelligence in Mental Health and the Biases of Language Based Models. _PLOS ONE, 15_(12), e0240376. [https://doi.org/10.1371/journal.pone.0240376](https://doi.org/10.1371/journal.pone.0240376)
*   Welty, C., Paritosh, P., & Aroyo, L. (2019). Metrology for AI: From Benchmarks to Instruments. _ArXiv_. [https://arxiv.org/abs/1911.01875v1](https://arxiv.org/abs/1911.01875v1)
*   Wesley, A. M., & Matisziw, T. C. (2021). Methods for Measuring Geodiversity in Large Overhead Imagery Datasets. _IEEE Access, 9_, 100279‚Äì100293. [https://doi.org/10.1109/ACCESS.2021.3096034](https://doi.org/10.1109/ACCESS.2021.3096034)
*   Zanella-B√©guelin, S., Wutschitz, L., Tople, S., R√ºhle, V., Paverd, A., Ohrimenko, O., K√∂pf, B., & Brockschmidt, M. (2020). Analyzing Information Leakage of Updates to Natural Language Models. _Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security_, 363‚Äì375. [https://doi.org/10.1145/3372297.3417880](https://doi.org/10.1145/3372297.3417880)
*   Zhong, R., Chen, Y., Patton, D., Selous, C., & McKeown, K. (2019). Detecting and Reducing Bias in a High Stakes Domain. _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, 4765‚Äì4775. [https://doi.org/10.18653/v1/D19-1483](https://doi.org/10.18653/v1/D19-1483)

### **c. Technical Audits**

This subsection includes works that employ technical audit-style investigations (e.g., Buolamwini & Gebru, 2018; Raji et al, 2020) of particular datasets.¬†

*   Babaeianjelodar, M., Lorenz, S., Gordon, J., Matthews, J., & Freitag, E. (2020). Quantifying Gender Bias in Different Corpora. _Companion Proceedings of the Web Conference 2020_, 752‚Äì759. [https://doi.org/10.1145/3366424.3383559](https://doi.org/10.1145/3366424.3383559)
*   Bountouridis, D., Makhortykh, M., Sullivan, E., Harambam, J., Tintarev, N., & Hauff, C. (2019). Annotating Credibility: Identifying and Mitigating Bias in Credibility Datasets. _ROME 2019 - Workshop on Reducing Online Misinformation Exposure_. [https://rome2019.github.io/papers/Bountouridis_etal_ROME2019.pdf](https://rome2019.github.io/papers/Bountouridis_etal_ROME2019.pdf)
*   Buolamwini, J., & Gebru, T. (2018, January). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. _Conference on fairness, accountability and transparency_, 77-91. [https://www.media.mit.edu/publications/gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification/](https://www.media.mit.edu/publications/gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification/)
*   Costanza-Chock, S., Raji, I. D., & Buolamwini, J. (2022). Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 1571‚Äì1583. [https://doi.org/10.1145/3531146.3533213](https://doi.org/10.1145/3531146.3533213)
*   Davidson, T., Bhattacharya, D., & Weber, I. (2019). Racial Bias in Hate Speech and Abusive Language Detection Datasets. _ArXiv_. [http://arxiv.org/abs/1905.12516](http://arxiv.org/abs/1905.12516)
*   ‚Äã‚ÄãDulhanty, C., & Wong, A. (2019). Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets. _ArXiv_. [http://arxiv.org/abs/1905.01347](http://arxiv.org/abs/1905.01347)
*   Dulhanty, C., & Wong, A. (2020). Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification. _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society_, 244‚Äì250. [https://doi.org/10.1145/3375627.3375875](https://doi.org/10.1145/3375627.3375875)
*   Dulhanty, C. (2020). Issues in Computer Vision Data Collection: Bias, Consent, and Label Taxonomy \[University of Waterloo\]. [https://uwspace.uwaterloo.ca/handle/10012/16414](https://uwspace.uwaterloo.ca/handle/10012/16414)
*   Heinzerling, B. (2019, July 21). NLP‚Äôs Clever Hans Moment has Arrived. _Benjamin Heinzerling_. [https://bheinzerling.github.io/post/clever-hans/](https://bheinzerling.github.io/post/clever-hans/)
*   Hutchinson, B., Prabhakaran, V., Denton, E., Webster, K., Zhong, Y., & Denuyl, S. (2020). Social Biases in NLP Models as Barriers for Persons with Disabilities. _ArXiv_. [http://arxiv.org/abs/2005.00813](http://arxiv.org/abs/2005.00813)
*   Klockmann, V., von Schenk, A., & Villeval, M. C. (2021). Artificial Intelligence, Ethics, and Diffused Pivotality. _Working Paper Series, GATE_. [https://ssrn.com/abstract=3853829](https://ssrn.com/abstract=3853829)
*   Luccioni, A., & Viviano, J. (2021). What‚Äôs in the Box? A Preliminary Analysis of Undesirable Content in the Common Crawl Corpus. _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing_, 182-189. [https://aclanthology.org/2021.acl-short.24.pdf](https://aclanthology.org/2021.acl-short.24.pdf)
*   Mecati, M., Cannav√≤, F. E., Vetr√≤, A., & Torchiano, M. (2020). Identifying Risks in Datasets for Automated Decision‚ÄìMaking. In G. Viale Pereira, M. Janssen, H. Lee, I. Lindgren, M. P. Rodr√≠guez Bol√≠var, H. J. Scholl, & A. Zuiderwijk (Eds.), _Electronic Government_ (pp. 332‚Äì344). Springer International Publishing. [https://doi.org/10.1007/978-3-030-57599-1_25](https://doi.org/10.1007/978-3-030-57599-1_25)
*   Raji, I. D., & Fried, G. (2021). About Face: A Survey of Facial Recognition Evaluation. _ArXiv_. [http://arxiv.org/abs/2102.00813](http://arxiv.org/abs/2102.00813)
*   Raji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., & Denton, E. (2020). Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing. _ArXiv_. [http://arxiv.org/abs/2001.00964](http://arxiv.org/abs/2001.00964)
*   Rambachan, A., & Roth, J. (2020). Bias In, Bias Out? Evaluating the Folk Wisdom. _ArXiv_. [https://doi.org/10.4230/LIPIcs.FORC.2020.6](https://doi.org/10.4230/LIPIcs.FORC.2020.6)
*   Shankar, S., Halpern, Y., Breck, E., Atwood, J., Wilson, J., & Sculley, D. (2017). No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World. _ArXiv_. [https://arxiv.org/abs/1711.08536](https://arxiv.org/abs/1711.08536)
*   Vidgen, B., & Derczynski, L. (2020). Directions in Abusive Language Training Data: Garbage In, Garbage Out. _ArXiv_. [https://arxiv.org/abs/2004.01670](https://arxiv.org/abs/2004.01670)
*   Wang, T., Zhao, J., Yatskar, M., Chang, K.-W., & Ordonez, V. (2019). Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations. _2019 IEEE/CVF International Conference on Computer Vision (ICCV)_. [https://doi.org/10.1109/ICCV.2019.00541](https://doi.org/10.1109/ICCV.2019.00541)

### **d. Visual & Artistic Approaches to Datasets**

‚Äã‚ÄãThis final subsection assembles artistic and visual approaches/formats for the analysis of datasets.

*   Baker, D. (2022). _Datasets Have Worldviews_ \[Website\]. PAIR Explorables. [https://pair.withgoogle.com/explorables/dataset-worldviews/](https://pair.withgoogle.com/explorables/dataset-worldviews/)
*   Crawford, K. & Paglen, T. (2019). _Training Humans_ \[Large-scale exhibition\]. Fondazione Prada, Milan, 2019-2020. [https://www.fondazioneprada.org/project/training-humans/?lang=en](https://www.fondazioneprada.org/project/training-humans/?lang=en)Publication: [Training Humans Book](https://bookshop.fondazioneprada.org/shop/UIShop/Shop_DettaglioArticolo.aspx?idshoparticolo=117)
*   Dewey-Hagbord, H. (2019). _‚Äã‚ÄãHow Do You See Me?_ \[Adversarial processes\]. The Photographer‚Äôs Gallery, London, UK. [https://thephotographersgallery.org.uk/whats-on/heather-dewey-hagborg-how-do-you-see-me](https://thephotographersgallery.org.uk/whats-on/heather-dewey-hagborg-how-do-you-see-me)
*   Malev√©, N. (2019)._12 hours of ImageNet_ \[Computer script\]. The Photographer‚Äôs Gallery, London, UK. [https://thephotographersgallery.org.uk/whats-on/exhibiting-imagenet](https://thephotographersgallery.org.uk/whats-on/exhibiting-imagenet)
*   Paglen, T. and Crawford, K. (2019). _Imagenet Roulette_ \[Software program\]. Launched at SXSW. [https://www.youtube.com/watch?v=S0yEPZJnvgs](https://www.youtube.com/watch?v=S0yEPZJnvgs)
*   Pipkin, E. (2020). _On Lacework: Watching an Entire Machine-Learning Dataset. Unthinking Photography_. [https://unthinking.photography/articles/on-lacework](https://unthinking.photography/articles/on-lacework)
*   Ridler, A. (2018). _Myriad (Tulips)_ \[C-type digital prints with handwritten annotations, magnetic paint, magnets\]. Barbican Centre, London, UK. [http://annaridler.com/myriad-tulips](http://annaridler.com/myriad-tulips)