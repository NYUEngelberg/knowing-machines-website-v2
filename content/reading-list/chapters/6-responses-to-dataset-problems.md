Here we assemble literature that proposes responses to commonly identified sociotechnical problems with ML datasets. Most of the articles in this vein focused on technical responses to addressing bias (writ broadly), while a few address other concerns such as privacy and security. We do not necessarily endorse these approaches; rather, this is a loose mapping of emerging areas of focus in response to problems. Note that there is some overlap with the readings suggested in Section 5, as many of these papers investigate particular datasets; however, the papers listed here emphasize approaches to addressing specific problems. 

### **a. General Recommendations for Dataset Design**

This subsection covers miscellaneous broad recommendations for the creation of fairer and more accountable datasets. 

*   Andrus, M., & Villeneuve, S. (2022). Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 1709–1721. [https://doi.org/10.1145/3531146.3533226](https://doi.org/10.1145/3531146.3533226)
*   Bilstrup, K.-E. K., Kaspersen, M. H., Assent, I., Enni, S., & Petersen, M. G. (2022). From Demo to Design in Teaching Machine Learning. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 2168–2178. [https://doi.org/10.1145/3531146.3534634](https://doi.org/10.1145/3531146.3534634)
*   Bowman, S. R., & Dahl, G. E. (2021). What Will it Take to Fix Benchmarking in Natural Language Understanding? _NAACL_. [https://doi.org/10.18653/V1/2021.NAACL-MAIN.385](https://doi.org/10.18653/V1/2021.NAACL-MAIN.385)
*   Boyd, K. (2022). Designing Up with Value-Sensitive Design: Building a Field Guide for Ethical ML Development. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 2069–2082. [https://doi.org/10.1145/3531146.3534626](https://doi.org/10.1145/3531146.3534626)
*   Kiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., Vidgen, B., Prasad, G., Singh, A., Ringshia, P., Ma, Z., Thrush, T., Riedel, S., Waseem, Z., Stenetorp, P., Jia, R., Bansal, M., Potts, C., & Williams, A. (2021). Dynabench: Rethinking Benchmarking in NLP. _NAACL_. [https://doi.org/10.18653/V1/2021.NAACL-MAIN.324](https://doi.org/10.18653/V1/2021.NAACL-MAIN.324)
*   Panch, T., Pollard, T. J., Mattie, H., Lindemer, E., Keane, P. A., & Celi, L. A. (2020). “Yes, But Will It Work for My Patients?” Driving Clinically Relevant Research with Benchmark Datasets. _Npj Digital Medicine_, _3_(1), 1–4. [https://doi.org/10.1038/s41746-020-0295-6](https://doi.org/10.1038/s41746-020-0295-6)
*   Peng, K., Mathur, A., & Narayanan, A. (2021). Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers. _ArXiv_. [http://arxiv.org/abs/2108.02922](http://arxiv.org/abs/2108.02922)
*   Rogers, A. (2020). Changing the World by Changing the Data. _ArXiv_. [https://arxiv.org/abs/2105.13947](https://arxiv.org/abs/2105.13947)
*   Rolf, E., Worledge, T., Recht, B., & Jordan, M. I. (2021). Representation Matters: Assessing the Importance of Subgroup Allocations in Training Data. _ArXiv_. [https://arxiv.org/abs/2103.03399](https://arxiv.org/abs/2103.03399)
*   Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & Vertesi, J. (2019). Fairness and Abstraction in Sociotechnical Systems. _Proceedings of the Conference on Fairness, Accountability, and Transparency_, 59–68. [https://doi.org/10.1145/3287560.3287598](https://doi.org/10.1145/3287560.3287598)
*   Suresh, H., Movva, R., Lee Dogan, A., Bhargava, D., Isadora, C., Martinez Cuba, A., Taurino, G., So, W., & D’Ignazio, C. (2022). Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Femicide Counterdata Collection. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 667-678. [https://doi.org/10.1145/3531146.3533132](https://doi.org/10.1145/3531146.3533132)
*   Stasaski, K., Yang, G. H., & Hearst, M. A. (2020). More Diverse Dialogue Datasets via Diversity-Informed Data Collection. _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, 4958–4968. [https://doi.org/10.18653/v1/2020.acl-main.446](https://doi.org/10.18653/v1/2020.acl-main.446)

### **b. Creating New Datasets and/or Remediation of Existing Datasets**

This subsection includes articles that either remediate specific existing datasets or detail the creation of alternative datasets to address identified privacy and bias issues.

*   Asano, Y., Rupprecht, C., Zisserman, A., & Vedaldi, A. (2021). PASS: An ImageNet Replacement for Self-Supervised Pretraining Without Humans. _ArXiv_. [https://arxiv.org/abs/2109.13228](https://arxiv.org/abs/2109.13228)
*   Brown, H., Lee, K., Mireshghallah, F., Shokri, R., & Tramèr, F. (2022). What Does it Mean for a Language Model to Preserve Privacy? _2022 ACM Conference on Fairness, Accountability, and Transparency_, 2280–2292. [https://doi.org/10.1145/3531146.3534642](https://doi.org/10.1145/3531146.3534642)
*   Cai, W., Encarnacion, R., Chern, B., Corbett-Davies, S., Bogen, M., Bergman, S., & Goel, S. (2022). Adaptive Sampling Strategies to Construct Equitable Training Datasets. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 1467–1478. [https://doi.org/10.1145/3531146.3533203](https://doi.org/10.1145/3531146.3533203)
*   Jernite, Y., Nguyen, H., Biderman, S., Rogers, A., Masoud, M., Danchev, V., Tan, S., Luccioni, A. S., Subramani, N., Johnson, I., Dupont, G., Dodge, J., Lo, K., Talat, Z., Radev, D., Gokaslan, A., Nikpoor, S., Henderson, P., Bommasani, R., & Mitchell, M. (2022). Data Governance in the Age of Large-Scale Data-Driven Language Technology. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 2206–2222. [https://doi.org/10.1145/3531146.3534637](https://doi.org/10.1145/3531146.3534637)
*   Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S., & Roth, D. (2018). Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences. _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1_, 252–262. [https://doi.org/10.18653/v1/N18-1023](https://doi.org/10.18653/v1/N18-1023)
*   Yang, K., Qinami, K., Fei-Fei, L., Deng, J., & Russakovsky, O. (2020). Towards Fairer Datasets: Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy. _FAT\* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency_, 547-558. [https://doi.org/10.1145/3351095.3375709](https://doi.org/10.1145/3351095.3375709)
*   Yang, K., Yau, J., Fei-Fei, L., Deng, J., & Russakovsky, O. (2021). A Study of Face Obfuscation in ImageNet. _ArXiv_. [https://arxiv.org/abs/2103.06191](https://arxiv.org/abs/2103.06191)
*   Zellers, R., Bisk, Y., Schwartz, R., & Choi, Y. (2018). SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. _ArXiv_. [https://arxiv.org/abs/1808.05326v1](https://arxiv.org/abs/1808.05326v1)

### **c. Data Annotation Workflows**

Articles in this subsection address biased machine learning datasets by proposing changes to data annotation processes.

*   Barbosa, N. M., & Chen, M. (2019). Rehumanized Crowdsourcing: A Labeling Framework Addressing Bias and Ethics in Machine Learning. _Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems_, 1–12. [http://doi.org/10.1145/3290605.3300773](https://doi.org/10.1145/3290605.3300773)
*   Beretta, E., Vetrò, A., Lepri, B., & Martin, J. C. D. (2021). Detecting Discriminatory Risk Through Data Annotation Based on Bayesian Inferences. _FAccT_. [https://doi.org/10.1145/3442188.3445940](https://doi.org/10.1145/3442188.3445940)
*   Beretta, E., Vetrò, A., Lepri, B., & De Martin, J. C. (2019). Ethical and Socially-Aware Data Labels. In J. A. Lossio-Ventura, D. Muñante, & H. Alatrista-Salas (Eds.), _Information Management and Big Data_, 320–327. Springer International Publishing. [https://doi.org/10.1007/978-3-030-11680-4_30](https://doi.org/10.1007/978-3-030-11680-4_30)
*   Rateike, M., Majumdar, A., Mineeva, O., Gummadi, K. P., & Valera, I. (2022). Don’t Throw it Away! The Utility of Unlabeled Data in Fair Decision Making. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 1421–1433. [https://doi.org/10.1145/3531146.3533199](https://doi.org/10.1145/3531146.3533199)

### **d. Data Augmentation**

Articles in this subsection offer approaches to reducing bias in datasets by changing their composition via techniques such as oversampling or the use of synthetic/pseudo-data.

*   Iosifidis, V., & Ntoutsi, E. (2018). Dealing with Bias via Data Augmentation in Supervised Learning Scenarios. [http://ceur-ws.org/Vol-2103/paper_5.pdf](http://ceur-ws.org/Vol-2103/paper_5.pdf)
*   Pastaltzidis, I., Dimitriou, N., Quezada-Tavarez, K., Aidinlis, S., Marquenie, T., Gurzawska, A., & Tzovaras, D. (2022). Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 2302–2314. [https://doi.org/10.1145/3531146.3534644](https://doi.org/10.1145/3531146.3534644)
*   Sharma, S., Zhang, Y., Ríos Aliaga, J. M., Bouneffouf, D., Muthusamy, V., & Varshney, K. R. (2020). Data Augmentation for Discrimination Prevention and Bias Disambiguation. _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society_, 358–364. [https://doi.org/10.1145/3375627.3375865](https://doi.org/10.1145/3375627.3375865)
*   Tomalin, M., Byrne, B., Concannon, S., Saunders, D., & Ullmann, S. (2021). The Practical Ethics of Bias Reduction in Machine Translation: Why Domain Adaptation is Better than Data Debiasing. _Ethics and Information Technology, 23_, 419-433. [https://doi.org/10.1007/s10676-021-09583-1](https://doi.org/10.1007/s10676-021-09583-1)

### **e. Bias Detection**

This subsection gathers tools and approaches for detecting bias in datasets.

*   Chapman, A., Grylls, P., Ugwudike, P., Gammack, D., & Ayling, J. (2022). A Data-Driven Analysis of the Interplay Between Criminology Theory and Predictive Policing Algorithms. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 36-45. [https://doi.org/10.1145/3531146.3533071](https://doi.org/10.1145/3531146.3533071)
*   Goyal, P., Romero Soriano, A., Hazirbas, C., Levent, S., & Usunier, N. (2022). Fairness Indicators for Systematic Assessments of Visual Feature Extractors. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 70-88. [https://doi.org/10.1145/3531146.3533074](https://doi.org/10.1145/3531146.3533074)
*   Harris, C., Halevy, M., Howard, A., Bruckman, A., & Yang, D. (2022). Exploring the Role of Grammar and Word Choice in Bias Toward African American English (AAE) in Hate Speech Classification. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 789-798. [https://doi.org/10.1145/3531146.3533144](https://doi.org/10.1145/3531146.3533144)
*   Hu, X., Wang, H., Vegesana, A., Dube, S., Yu, K., Kao, G., Chen, S.-H., Lu, Y.-H., Thiruvathukal, G. K., & Yin, M. (2020). Crowdsourcing Detection of Sampling Biases in Image Datasets. _Proceedings of The Web Conference 2020_, 2955–2961. [https://doi.org/10.1145/3366423.3380063](https://doi.org/10.1145/3366423.3380063)
*   Leavy, S., Meaney, G., Wade, K., & Greene, D. (2020). Mitigating Gender Bias in Machine Learning Data Sets. In L. Boratto, S. Faralli, M. Marras, & G. Stilo (Eds.), _Bias and Social Aspects in Search and Recommendation_, 12–26. Springer International Publishing. [https://doi.org/10.1007/978-3-030-52485-2_2](https://doi.org/10.1007/978-3-030-52485-2_2)
*   Pahl, J., Rieger, I., Mӧller, A., Wittenberg, T., & Schmid, U. (2022). Female, White, 27? Bias Evaluation on Data and Algorithms for Affect Recognition in Faces. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 973-987. [https://doi.org/10.1145/3531146.3533159](https://doi.org/10.1145/3531146.3533159)
*   Srinivasan, R., & Chander, A. (n.d.). Understanding Bias in Datasets using Topological Data Analysis. 7. [http://ceur-ws.org/Vol-2419/paper_9.pdf](http://ceur-ws.org/Vol-2419/paper_9.pdf)
*   Verma, S., Ernst, M., & Just, R. (2021). Removing Biased Data to Improve Fairness and Accuracy. _ArXiv_. [https://arxiv.org/abs/2102.03054](https://arxiv.org/abs/2102.03054)
*   Wang, A., Barocas, S., Laird, K., & Wallach, H. (2022). Measuring Representational Harms in Image Captioning. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 324-335. [https://doi.org/10.1145/3531146.3533099](https://doi.org/10.1145/3531146.3533099)
*   Wang, A., Narayanan, A., & Russakovsky, O. (2020). REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets. _ECCV_, 733-751. [https://doi.org/10.1007/978-3-030-58580-8_43](https://doi.org/10.1007/978-3-030-58580-8_43)
*   Wang, A., Ramaswamy, V. V., & Russakovsky, O. (2022). Towards Intersectionality in Machine Learning: Including More Identities, Handling Underrepresetation, and Performing Evaluation. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 336-349. [https://doi.org/10.1145/3531146.3533101](https://doi.org/10.1145/3531146.3533101)
*   Zamfirescu-Pereira, J. D., Chen, J., Wen, E, Koenecke, A., Garg, N., & Pierson, E. (2022) Trucks Don’t Mean Trump: Diagnosing Human Error in Image Analysis. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 799-813. [https://doi.org/10.1145/3531146.3533145](https://doi.org/10.1145/3531146.3533145)

### **f. Algorithms to Debias Datasets or Mitigate Bias**

Research in this subsection deploys algorithmic techniques to either debias datasets before training ML models on them or intervene to mitigate bias after training.

*   Abbasi-Sureshjani, S., Raumanns, R., Michels, B. E. J., Schouten, G., & Cheplygina, V. (2020). Risk of Training Diagnostic Algorithms on Data with Demographic Bias. In J. Cardoso et al (Eds.), _Interpretable and Annotation-Efficient Learning for Medical Image Computing_, 183–192. Springer. [https://doi.org/10.1007/978-3-030-61166-8_20](https://doi.org/10.1007/978-3-030-61166-8_20)
*   Almuzaini, A. A., Bhatt, C. A., Pennock, D. M., & Singh, V. K. (2022). ABCinML: Anticipatory Bias Correction in Machine Learning Applications. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 1552–1560. [https://doi.org/10.1145/3531146.3533211](https://doi.org/10.1145/3531146.3533211)
*   Anahideh, H., Asudeh, A., & Thirumuruganathan, S. (2021). Fair Active Learning. _ArXiv_. [http://arxiv.org/abs/2001.0179](http://arxiv.org/abs/2001.0179)
*   Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., & Kalai, A. (2016). Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. _ArXiv_. [http://arxiv.org/abs/1607.06520](http://arxiv.org/abs/1607.06520)
*   Hendricks, L. A., Burns, K., Saenko, K., Darrell, T., & Rohrbach, A. (2018). Women Also Snowboard: Overcoming Bias in Captioning Models. _ECCV_, 771–787. [https://openaccess.thecvf.com/content_ECCV_2018/html/Lisa_Anne_Hendricks_Women_also_Snowboard_ECCV_2018_paper.html](https://openaccess.thecvf.com/content_ECCV_2018/html/Lisa_Anne_Hendricks_Women_also_Snowboard_ECCV_2018_paper.html)
*   Lum, K., Zhang, Y., & Bower, A. (2022). De-Biasing “Bias” Measurement. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 379-389. [https://doi.org/10.1145/3531146.3533105](https://doi.org/10.1145/3531146.3533105)
*   Reimers, C., Bodesheim, P., Runge, J., & Denzler, J. (2021). Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing. _ArXiv_. [https://arxiv.org/abs/2103.06179](https://arxiv.org/abs/2103.06179)
*   Ryu, H. J., Mitchell, M., & Adam, H. (2017). InclusiveFaceNet: Improving Face Attribute Detection with Race and Gender Diversity. _ArXiv_. [https://arxiv.org/abs/1712.00193](https://arxiv.org/abs/1712.00193)
*   Schick, T., Udupa, S., & Schütze, H. (2021). Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP. _ArXiv_. [https://arxiv.org/abs/2103.00453](https://arxiv.org/abs/2103.00453)
*   Sikdar, S., Lemmerich, F., & Strohmaier, M. (2022). GetFair: Generalized Fairness Tuning of Classification Models. _2022 ACM Conference on Fairness, Accountability, and Transparency_, 289-299. [https://doi.org/10.1145/3531146.3533094](https://doi.org/10.1145/3531146.3533094)
*   Zhao, J., Wang, T., Yatskar, M., Ordonez, V., & Chang, K.-W. (2017). Men Also Like Shopping: Reducing Gender Bias Amplification Using Corpus-level Constraints. _ArXiv_. [http://arxiv.org/abs/1707.09457](http://arxiv.org/abs/1707.09457)